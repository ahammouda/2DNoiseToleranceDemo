% algorithm2e macros:
\SetKwFor{Procedure}{procedure}{is}{end procedure}
\SetKwInOut{Requires}{Requires} % precondition (assumed)
\SetKwInOut{Ensures}{Ensures} % postcondition (guaranteed)
\SetKwInOut{Uses}{Uses} % variables used
\SetKw{DownTo}{downto}
% for nondeterministic choice:
%\SetKwIF{Choose}{ChooseOr}{ChooseLast}{choose}{}{or}{or}{end}
% \SetKwSwitch{Choose}{ChooseCase}{ChooseOther}{choose}{}{when}{}{}
\SetKwFor{WhileChoose}{while}{do choose}{}
\SetKwFor{While}{while}{do}{}
\SetKwFor{ForAny}{for any}{do}{}
\SetKwIF{When}{Unused}{Unused}{when}{}{}{}{}

\newcommand{\nxl}{\textup{\textsf{nxl}}} % number of local cells (COLUMNS) excluding ghosts
\newcommand{\nyl}{\textup{\textsf{nyl}}} % number of local cells (ROWS) excluding ghosts
\newcommand{\tm}{ \text{$t_{max}$} }
\newcommand{\av}{ \mathbf{a} }
\newcommand{\uv}{ \mathbf{u} }
\newcommand{\bv}{ \mathbf{b} }
\newcommand{\pv}{ \mathbf{p} }
\newcommand{\orig} {\mathcal{O}}

\newcommand{\rnt}{ R_{-\theta} }
\newcommand{\rt}{ R_{\theta} }

\newcommand{\la}{\leftarrow} % for assignments
\newcommand{\ra}{\rightarrow} % for assignments

\newcommand{\dx}{\textsf{dx}}
\newcommand{\dy}{\textsf{dy}}

\newcommand{\data}{\textup{\textsf{data}}}
\newcommand{\psendf}{\textit{postSend}}
\newcommand{\precvf}{\textit{postRecv}}
\newcommand{\psend}[2]{\psendf(#1,#2)}
\newcommand{\precv}[2]{\precvf(#1,#2)}
\newcommand{\pid}{\textup{\textsf{pid}}} % my rank


\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{classic_division.eps}
  \caption{4 process decomposition of a 22x16 stencil.  Process labels start in the top left and proceed clockwise.  Local
  variables of state are also illustrated for indexing a 2D array, 0,1,$B_x^1$,etc.}
  \label{fig:basic}
\end{figure}

\section{Basic Terminology And Motivation}\label{sec:problem_statement}
% Stencil DEF as dependency structure
The solution of partial differential equations (PDEs)
is a well known problem in computing addressed by many
numerical methods.
To solve a PDE numerically, a method must choose a discrete
form of its equation.
A key characteristic of the discrete form chosen is its
 {\it stencil}.
%   characterizes the discretization used for the numerical
% solutions of partial differential equations (PDEs).
A stencil
 defines a dependency structure that constrains
the execution of the method.  The dynamics
of this dependency structure is the topic of this section.
These dynamics further define a central performance bottleneck in
solving PDEs on parallel processors which is the motivation
for the algorithm
presented in the sections that follow.

Figure~\ref{fig:basic} illustrates an example of a {\it stencil}
for a 2D PDE at grid cell $(B_x^1,B_y^1)$ of process 4.
At cell $(B_x^1,B_y^1)$ there is a single blue circle surrounded by
4 circles.
This structure is known as a {\it 5-point stencil} for the 5 blue
circles in the image, and it characterizes a well known
discretization for the solution of 2D PDEs.
The grid cell containing the
center circle is the spatially dependent value of the PDE at a given point in
 time.  This value can undergo an {\it update} as a function of
 the values of the
 grid cells in the adjacent circles for that same point in time.
  We therefore refer to those adjacent grid cells as {\it
 dependencies}.
 The unit of time taken for a
 cell to update the value of its PDE is referred to as the
 {\it timestep} of that cell.  Given a well defined stencil,
 the relationship between the
  timestep of a cell and its adjacent cells can be studied
 independently of the values of the underlying PDE.

\begin{wrapfigure}{r}{0.5\linewidth}
  \centering
  \includegraphics[width=0.5\textwidth]{timeStepDependencies.eps}
  \caption{An illustration of grid cell timesteps, and their
  relationship to
  stencil dependencies.  In cell $(B_x^1,B_y^1)$ dependencies are
  valid and an update of the cell can be performed.  In cell $(B_x^1,1)$ the opposite
  is true.  Transparent blue and red circles are overlaid respectively
  for valid and invalid dependencies.}
  \label{fig:timestep}
\end{wrapfigure}

% Implications of Dependency Structure For Single Proc
Figure~\ref{fig:timestep} illustrates the timestep at each cell of process 4
from figure~\ref{fig:basic},
for some arbitrary point during the computation of a
PDE's solution.  It
demonstrates how the spatial dependencies of each cell can change
with time.  The cell
$(B_x^1,B_y^1)$ has a time step of 3, and so do all of its adjacent cells.
When a cell has neighbors above, below, left, and right of it which contain
values computed from the same timestep, we'll say the cell's
dependencies are {\it valid} at a given point in time.  When any one of its
neighbors does not contain a value from the same timestep, then its
dependencies are {\it invalid}.  An example of a cell with
invalid dependencies is given in
cell $(B_x^1,1)$ in figure~\ref{fig:timestep}, where the value of the PDE
is only completed up to the 2nd timestep, and it's neighboring cells
contain values from different timesteps.  Given
that timesteps increment for every update,
a cell with {\it valid} dependencies in
one timestep may acquire {\it invalid}
dependencies immediately following an update, and any
numerical solver is constrained by this.

% Implications of dependency structure for multi-proc
The constraints resulting from the stencil's dependency structure
further effects the performance of a
parallelization strategy for computing a 2D PDE.  A typical
strategy starts with the definition of a global 2D area over which
a PDE is being solved, followed by a subdivision of that area
into separate 2D grids for each
processing unit to manage.  Figure~\ref{fig:basic} illustrates the general principle for
4 processing units, each with separate
11x8 grid portions of the total 22x16 global grid size.
For a parallel numerical method that cannot exploit shared memory,
the 11x8 grids are insufficient to capture interprocess
dependencies, requiring
13x10 grid cells for each process subdivision illustrated.
The additional gray ({\it ghost}) cells which account for the increased
dimensions are redundant relative to the global PDE solution, and they
hold information which originates from the blue ({\it border}) cells
held by the adjacent processes.  To give a precise example in
figure~\ref{fig:basic},
cells $(1,1) - (B_x^1,1)$ of process 1 need to be
reflected in cells $(1,B_y^1+1) - (B_x^1, B_y^1+1)$ on process 4 in order
for process 4 to compute cells $(1,B_y^1) - (B_x^1, B_y^1)$.  The analogous
constraint is true for the other border/ghost cell pairs for each of the
processes.
This data sharing between processes defines an additional process level
 dependency that is a direct
  result of the cell level dependency structure introduced by a 5 point
  stencil.

 % Classification of grid regions according to interprocess dependency
This dependency structure between processes is known as a
{\it nearest neighbor} dependency structure, and divides
each process' local grid into 9 distinct {\it regions} depending on
their degree of interprocess dependence.  Continuing to use process 4
in figure~\ref{fig:basic} as an example, the
regions are the green
shaded {\it interior region}, and the 8 blue shaded border regions.  The interior region depends only on border
regions, and is therefore relatively independent of neighbor processes.
The border regions have 2 types of dependence.  The first type depends on only one neighbor process.  This is
given by cell ranges on the 4 sides of the 2D grid: $(2,1) - (B_x^1 - 1, 1)$, $(1,2) - (1,B_y^1 - 1)$,
$(2,B_y^1) - (B_x^1 - 1, B_y^1)$, and $(B_x^1,2) - (B_x^1,B_y^1 - 1)$.  In these ranges only the
border cells received from a single adjacent process are
needed to successfully compute grid cells at each point.  The other type of
region depends on 2 processes and exists exclusively
in the corners of the 2D grid at cells: $(1,1)$, $(1,B_y^1)$, $(B_x^1,B_y^1)$,
and $(B_x^1,1)$.

% Performance bottleneck introduced by interprocess DEP
Given the 9 static regions described, if a process wishes to execute a complete
timestep for each cell in its grid, it must synchronize the pace of its timesteps
with each neighbor process .
If synchronizations are expensive this creates a performance bottleneck, and
it becomes beneficial
to develop strategies to relax these synchronization requirements.
In order to maintain
valid stencil dependencies while relaxing synchronizations
requires relaxing the boundaries of the 9 classically
static regions of a 2D grid
on each process.  In the next section we'll describe how this is done, before
developing a concise approach for performing region updates
that guarantee valid cell level dependencies.
Ù’
\section{State Variables For Region Decomposition}\label{sec:state}

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=.8\textwidth]{variable_divisions.eps}
    \caption{State maintained for the purpose of extrapolating all area boundaries}
    \label{fig:state_labels}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{prior_art_relable.eps}
    \caption{Region labels, each given by 2 digits in the form $a_xa_y$ throughout the rest of the document.
    $a_xa_y$ additionally provides a shorthand for the vector $[a_x\;\;\; a_y]^T$, and we therefore refer to
    these region labels as region vectors, or area vectors throughout the document.}\label{fig:region_vectors}
    \end{subfigure}
  %\includegraphics[width=0.4\textwidth]{region_map.pdf}
  \caption{(\ref{fig:state_labels}) An illustration of state labels that allow each of the 9 regions to relax their
  boundaries.  (\ref{fig:region_vectors}) Shorthand labels for each region, also known as area vectors.}
  \label{fig:local_decomposition}
\end{figure}

Figure~\ref{fig:region_vectors} illustrates a concise labeling of
each of the 9 regions described above, whose boundaries have been
relaxed to arbitrary positions.  The regions correspond to our
prior explanations as follows:
\begin{itemize}
  \item $00$ is the interior region with no interprocess dependencies.
  \item $\{^-10,10,01,0^-1 \}$ is the set of border regions with single process dependencies.
  \item $\{^-1^-1,1^-1,11,^-11 \}$ is the set of corner regions with 2 process dependencies.
\end{itemize}
Region labels are also referred to as region vectors throughout this
paper, and the general form used above, $a_xa_y$, is
shorthand for the vector $[a_x,\; a_y]^T = \mathbf{a}^T$.

Figure~\ref{fig:state_labels} illustrates the minimal set of
variables needed to relax the boundaries of the regions
just described.
With these dynamic variables of state, each process has enough information
to ensure the grid cells
of each region have valid dependencies\footnote{
  This isn't proven formally at the moment, but is part of the research plan
}.
This is because each state variable captures information about the timestep,
and therefore dependency information of each region.  To illustrate this point,
consider the interior region, $00$, which
has no explicit dependencies.  Given that the number of grid cells in $00$
dominates the number of grid cells in the border region, we're
typically safe to continue shrinking this region
and updating it's grid cells regardless of whether neighbor
process data has been acquired for border updates.
Therefore, we define the maximal timestep possible as being
the uniform timestep reached by the interior most
region, $00$,

\begin{equation}
  \tm \equiv \text{The uniform timestep of each cell in region $00$}.
\end{equation}
From this definition, every region's timestep can be further defined relative to $\tm$ and the state
variables drawn in figure~\ref{fig:state_labels}.  For example, variable $x_{^-1}$ is the difference in the
 number of timesteps completed by the interior region and the number of timesteps completed at the leftmost edge
 of region $^-10$.  $x_{^-1}$ is also equal to the number of timesteps since the last time a ghost cell was received along
 that border.  Further, $\delta_{^-1^-1}$ can be thought of as the number of times
region $^-10$ or region $0^-1$ have been updated without a corresponding update to region $^-1^-1$, making $\delta_{^-1^-1}$
negative as illustrated.  Figure~\ref{fig:state_labels} also illustrates a positive
 $\delta_{1^-1}$ term.  This can occur when $00$ has updated several times without
any updates from regions $0^-1$ and $10$.  These intuitions are
summarized analytically for all the dynamic variables illustrated
in figure~\ref{fig:state_labels} as
\begin{equation}
  t(a_xa_y,i_x,i_y) = \tm +
  a_x \cdot x_{a_x} + a_y \cdot y_{a_y} -
  a_x \cdot i_x - a_y \cdot i_y +
  |a_x|\cdot|a_y|\cdot\delta_{a_xa_y},
  \label{eq:ts}
\end{equation}
where $t(a_xa_y,i_x,i_y)$ provides the timestep for region $a_xa_y$ in grid cell $(i_x,i_y)$.

The only variables equation~\eqref{eq:ts} doesn't capture from figure~\ref{fig:state_labels} are the
static boundary terms, denoted by the $B$ variables.  Each $B^{-1}$ term denotes a lower bound, and $B^1$
 term
denotes an upper bound for the grid cells along dimensions $x$ and $y$ which are
correspondingly subscripted $B_x$ and $B_y$.
  These terms will represent array indexing for a process, so $B^{-1} = 0$ and $B^1 > 0$
  for dimensions $x$ and $y$.

\begin{algorithm}[h]
  \setcounter{AlgoLine}{0}
  \Procedure{$update\_state(a_xa_y)$}{
    \When{$a_xa_y = 00$}{
      $x_{^-1} = x_{^-1} + 1$; $\;\; y_{^-1} = y_{^-1} + 1$\; \label{ln:001}
      $x_{^1} = x_{^1} - 1$; $y_{^1} = y_{^1} - 1$\;
      $\delta_{^-1^-1} = \delta_{^-1^-1} + 1$;
      $\delta_{1^-1} = \delta_{1^-1} + 1$;
      $\delta_{11} = \delta_{11} + 1$;
      $\delta_{^-11} = \delta_{^-11} + 1$\; \label{ln:00N}
    }
    \When{$a_xa_y = ^-1^-1$}{ \label{ln:n1n1}
      $\delta_{^-1^-1} = \delta_{^-1^-1} + 1$\;
    }
    \When{$a_xa_y = 1^-1$}{
      $\delta_{1^-1} = \delta_{1^-1} + 1$\;
    }
    \When{$a_xa_y = 11$}{
      $\delta_{11} = \delta_{11} + 1$\;
    }
    \When{$a_xa_y = ^-11$}{
      $\delta_{^-11} = \delta_{^-11} + 1$\; \label{ln:n11}
    }
    \When{$a_xa_y = ^-10$}{ \label{ln:n10}
      $\delta_{^-1^-1} = \delta_{^-1^-1} - 1$;  $\delta_{^-11} = \delta_{^-11} - 1$\;
      $x_{^-1} = x_{^-1} - 1$\;
    }
    \When{$a_xa_y = 10$}{
      $\delta_{11} = \delta_{11} - 1$; $\delta_{1^-1} = \delta_{1^-1} - 1$\;
      $x_{^1} = x_{^1} + 1$;
    }
    \When{$a_xa_y = 0^-1$}{
      $\delta_{^-1^-1} = \delta_{^-1^-1} - 1$;  $\delta_{1^-1} = \delta_{1^-1} - 1$\;
      $y_{^-1} = y_{^-1} - 1$\;
    }
    \When{$a_xa_y = 01$}{
      $\delta_{^-11} = \delta_{^-11} - 1$;   $\delta_{11} = \delta_{11} - 1$\;
      $y_{^1} = y_{^1} + 1$\; \label{ln:01}
    }
  }
  \caption{Procedure to update state values for each region $a_xa_y$}
  \label{alg:state_updates_basic}
\end{algorithm}

  While we've defined the state we must track for each region,
we have not yet shown how the state evolves to account for delays in synchronization.
The details will be explored more
  below, but for now consider that algorithm~\ref{alg:state_updates_basic} will
 be run for each region, whenever every cell in that region has
 undergone an update. Lines~\ref{ln:001}--~\ref{ln:00N} contracts the interior region's
 boundaries
 while simultaneously expanding the boundaries of each adjacent region.
 Lines~\ref{ln:n1n1}--~\ref{ln:n11}
 contracts the various corner regions, while expanding their neighbor
 regions, and lines~\ref{ln:n10}--~\ref{ln:01} performs the same
 complimentary contraction and expansion for border regions.
 Note finally that if algorithm~\ref{alg:state_updates_basic} is called
 for every region $a_xa_y$ for the same global timestep, the region boundaries
 will all return to where they have started with
 effectively no change:  For every decrement or increment of a
 variable of state in one region, there is a corresponding increment or decrement
 in the opposite direction in another.

Algorithm~\ref{alg:state_updates_basic} is verbose for clarity,
but for all other state manipulation we'll introduce a more compact
notation.  To achieve we build
 intersection points from the variables introduced.
For example,
where line $y = x_{^-1}$ intersects with line $x = y_{^-1}$ is
the point $[x_{^-1}, y_{^-1}]$.  In general we'll write,
\begin{equation} \label{eq:p_sm}
  p_{qp} \equiv [x_q, x_p].
\end{equation}
For $B$ terms,
\begin{equation}
  b_{qp} \equiv [B_x^q, B_y^p].
  \label{eq:b_sm}
\end{equation}
Finally, for vectors of the subscripts themselves,
\begin{equation}
  u_{qp} \equiv [q, p].
  \label{eq:u_sm}
\end{equation}

We'll operationalize these terms later, and state them in matrix form now \footnote{
    Defining equations~\ref{eq:p_sm}--~\ref{eq:u_sm} allows us
    to ignore the inner vectors and manipulate the following terms
    with matrix operations.  A more robust formulation may be to
    use tensors which is a current topic of investigation.
  }.  For boundary intersections,
\begin{equation}
  \mathbf{B} \equiv \begin{bmatrix}
    [B_x^{-1}, B_y^{-1}] & [B_x^{1}, B_y^{-1}] \\
    [B_x^{-1}, B_y^{1}] & [B_x^{1}, B_y^{1}]
    \end{bmatrix} \equiv \begin{bmatrix}
      b_{^-1^-1} & b_{1^-1}] \\
      b_{^-11} & b_{11}
      \end{bmatrix}
      \label{eq:B}
\end{equation}
For interior point intersections,
\begin{equation}
  \mathbf{P} \equiv \begin{bmatrix}
    [x_{^-1}, y_{^-1}] & [x_1, y_{^-1}] \\
    [x_{^-1}, y_1] & [x_1, y_1]
  \end{bmatrix} \equiv \begin{bmatrix}
    p_{^-1^-1} & p_{1^-1}] \\
    p_{^-11} & p_{11}
    \end{bmatrix}.
  \label{eq:P}
\end{equation}
For a subset of labeling terms which will be explored
more in the next section,
\begin{equation}
\mathbf{U} \equiv \begin{bmatrix}
  [-1, -1] & [1, -1] \\
  [-1, 1] & [1, 1]
  \end{bmatrix} \equiv \begin{bmatrix}
    u_{^-1^-1} & u_{1^-1}] \\
    u_{^-11} & u_{11}
    \end{bmatrix}.
  \label{eq:indices}
\end{equation}
Finally for delta terms, we'll write
\begin{equation}
  \mathbf{\Delta} \equiv \begin{bmatrix}
    \delta_{^-1^-1} & \delta_{1^-1} \\
    \delta_{^-11} & \delta_{11}
    \end{bmatrix}.
    \label{eq:D}
\end{equation}
% TODO: Might be worth motivating _where_ this procedure
%       falls in the overall flow of the algorithm we're designing
%       -- probably worthwhile when introducing classic algorithm
% \begin{enumerate}
%     \item Initiate communication between processes for border and corner regions
%     \item Update each cell in the interior region
%     \item $\forall a_xa_y \in \mathcal{R} - \{00\}$
%     \subitem {\it If communication, has completed}, update the border, and call $update\_state$
%         (procedure~\ref{alg:state_updates_basic})
%     \subitem Otherwise {\it do nothing}.
% \end{enumerate}
With this summary of state for the new boundaries of a process'
 grid, we have the tools to update each grid cell while
 ensuring valid dependency structures.
In the next section, a strategy for updating each region described
by this state will be presented.

\section{Uniform Region Updates}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[ht]{.5\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{minimal_tiles.eps}
    \caption{Illustration of the fundamental dimensions that define each region's boundary: $h,H,V$ and
    $h',H',V'$.  We'll show in this section how to derive both bounds from the same function}\label{subfig:mintile}
  \end{subfigure}
  ~
  \begin{subfigure}[ht]{.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{subdivision.eps}
    \caption{A subdivision of all regions by drawing a horizontal and vertical line through the origin
    $\mathcal{O}$ - note how each subdivision of each region looks very similar to the regions
    annotated in \ref{subfig:mintile}}
    \label{subfig:subdivision}
    \end{subfigure}
  \caption{An illustration of the minimal set of dimensions needed for each region to be described by the same
  bounds.}
  \label{fig:tiles}
\end{figure}

\begin{algorithm}[h]
  \setcounter{AlgoLine}{0}
  \For{$\textsf{tile} \in \{\text{The tiles found in region }\;a_xa_y\}$}{\label{ln:all_tiles}
    \For{$i_x \la 1$ \KwTo $H(\textsf{tile})$}{ \label{ln:H}
      \For{$i_y \la 1$ \KwTo $\text{\bf min}( V(\textsf{tile}),V(\textsf{tile}) - i_x + h(\textsf{tile}) )$}{
        \label{ln:V}
        $\text{{\it Update the value at grid cell} $(i_x,i_y)$ {\it 1 timestep}}$\;
      }
    }
  }
  \caption{Procedure to update each region $a_xa_y$ according to the number of unit tiles present}
  \label{alg:tile_update}
\end{algorithm}

While the 9 regions above do not share the same dependency structure with respect to neighbor process cells,
they do share the same geometric forms which are captured by the corner regions: The pentagon of region
$^-1^-1$ and the squares of regions $1^-1$, $11$, and $^-11$ in figure~\ref{subfig:mintile} for example.
  The figure further illustrates dimensions
$H,h$ and $V$ for regions $^-1^-1$ and $^-11$ which capture both the pentagon and square shapes.

Given these observations, the pseudocode
of algorithm~\ref{alg:tile_update} presents a
reasonable approach to iterating over the cells of each
region in a uniform way.
  In line~\ref{ln:all_tiles} of the pseudo code, we assume that the unit tiles
 illustrated in figure~\ref{subfig:mintile} can be derived as a function of
 the region $a_xa_y$, and then in lines \ref{ln:H}--\ref{ln:V},
  each tile dimension ($H,h$, and $V$) can
 be derived as a function of the tile specific to that region.
 This pseudocode is a very rough outline of
 what we develop formally below, but it serves as
 useful checkpoint in our intuition.

Figure~\ref{subfig:subdivision} provides an illustration of how
a uniform tile structure can be extracted from each region.
This figure rescales figure~\ref{subfig:mintile} slightly and draws
a subdivision of each region
by intersecting perpendicular lines through an origin term (black center dot)
given by,
\begin{equation}
  \mathcal{O} \equiv \bigl[x_{^-1} + \frac{x_1 - x_{^-1}}{2}, \;\;\; y_{^-1} + \frac{y_1 - y_{^-1}}{2} \bigl].
  \label{eq:O}
\end{equation}
The intersecting lines through this point, cut across the center of $00$ horizontally and vertically,
and cut across the other side regions
($01$,$0^-1$,$10$,$^-10$) either horizontally or vertically.  The blue dots then represent the
origins of each unit geometry which can be specified by $H$, $h$, and $V$ as illustrated
in figure~\ref{subfig:mintile}.

With this in mind, as long as we can concisely shift our origin
and bounds for each region, we can use the
uniform iterates provided by algorithm~\ref{alg:tile_update} for
every region and its subdivision.
We can do this via simple coordinate transformations,
and rotations.  Transformations are defined by
\begin{eqnarray}
  T_x(a_xa_y) &= B_x^1 \cdot \mathbf{max}(a_x,0) + (1 - |a_x|) \cdot \mathcal{O}_x\\
  T_y(a_xa_y) &= B_y^1 \cdot \mathbf{max}(a_y,0) + (1 - |a_y|) \cdot \mathcal{O}_y,
  \label{eq:transforms}
\end{eqnarray}
where $\mathcal{O}_x$ is the first term of definition \ref{eq:O}, and $\mathcal{O}_y$ the second term.  To
rotate our orientation in each region so that each region may share the same origin, we can employ the
standard rotation matrix in 2 dimensions,
\begin{equation}
  R_{\theta} \equiv \begin{bmatrix}
    \text{cos} (\theta) & - \text{sin}(\theta) \\
    \text{sin}(\theta) & \text{cos}(\theta)
    \end{bmatrix}.
\end{equation}
To employ rotations properly, we'll need to know what angles
of rotation are required in each region and for each subdivision.
  To relate these rotations to the bounds of
figure~\ref{subfig:mintile}, we'll develop a function
that maps these angles to the state matrices of
section~\ref{sec:state}.
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{.4\textwidth}
    \centering
    \includegraphics[width=.8\textwidth]{vectors.eps}
    \caption{Vectors and angles corresponding to each
    region, with color coded sets of angles from $0^o - 270^o$.  The
    angle is calculated relative to the unit vector of
    figure~\ref{subfig:unit}}
    \label{subfig:all_vectors}
  \end{subfigure}
  ~
  \begin{subfigure}[h]{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{unit_square.eps}
    \caption{Unit vector overlaid on coordinates
    corresponding to region labels}
    \label{subfig:unit}
    \end{subfigure}
  \caption{Region angles, and the unit vector}
  \label{fig:vectors}
\end{figure}

\subsection{Regional Coordinate Rotation Angles}

Figure~\ref{subfig:all_vectors} illustrates the number of rotations present in each
region, along
with their rotation angles and the vectors they're derived from.  In practice, we'll
want a procedure to
derive these angles using only the region vector, $a_xa_y$.  To do so, it's necessary
to understand how
all of the vectors drawn in figure~\ref{subfig:all_vectors} acquire their angle and
magnitude.
Figure~\ref{subfig:unit} explains this in part by
illustrating the labels of each
region overlaid on a
cartesian plain, where the labels assemble to form a unit square centered on
$[0, \;\; 0]$.
Further, the unit vector whose
tail to head goes from $[0, \;\; 0]$ to $[1, \;\; 1]$ provides an orientation from
which to measure the angles in figure~\ref{subfig:all_vectors}.
  The vector of figure~\ref{subfig:unit} will be
  called the {\it unit vector}.  Our procedure for uncovering angles
for each region can now proceed as follows:
\begin{enumerate}
  \item Find the vectors which map to our unit tiles
  \item Find the angle between each vector and the unit vector
  \item return the set of angles.
\end{enumerate}
Given that we know the angles we're looking for--enumerated in figure~\ref{subfig:all_vectors}--
it is straightforward to verify the derived procedure.
\begin{algorithm}[hb]
  \setcounter{AlgoLine}{0}
  \Procedure{$AngleSearch(a_xa_y \in \mathcal{R})$}{
    $\mathcal{S}' = \{\}$\;

    \For{$s \in \mathcal{S}_u $}{
      $\mathcal{S}' = \mathcal{S}' \cup \{ \mathbf{s} + [a_x,\;\; a_y]\}$ \label{ln:as3}
    }
    $\mathcal{S}' = \mathcal{S}' \cap \mathcal{R}$\;  \label{ln:as4}

    $\Theta = \{\}$\;

    \For{$\mathbf{s} \in \mathcal{S}' $}{
      $\mathbf{u} = \mathbf{s} - [a_x,\;\; a_y]$\;  \label{ln:as7}

      $\Theta = \Theta \cup atan2(\mathbf{u}\cdot\mathbf{v}^u, |\mathbf{u}^T || {\mathbf{v}^u}^T| )$\;  \label{ln:as8}
    }
    $\mathbf{return}\;\; \Theta$\;
  }
  \caption{Procedure, {\it AngleSearch} which takes as input $a_xa_y \in \mathcal{R}$,
  and returns the set $\Theta$ of all the angles associated with it.}
  \label{alg:angle_search}
\end{algorithm}
To find the angles enumerated in figure~\ref{subfig:all_vectors},
we formalize some notions of our unit labelings of each region:
\begin{flalign}
  \mathcal{R} \equiv &\;\; \{ ^-1^-1,0^-1,1^-1,^-10,00,10,^-11,01,11 \}\\
  \mathcal{S}_u \equiv &\;\; \{ ^-1^-1,1^-1,^-11,11 \}\\
  \mathbf{v}^u \equiv &\;\; [1, \;\; 1].
\end{flalign}
$\mathcal{R}$ is just the set of all possible regions where $a_xa_y \in \mathcal{R}$
always.  $\mathcal{S}_u$ is a subset
of $\mathcal{R}$, and provides a way to derive the angles of
figure~\ref{subfig:all_vectors} when taken together with
$\mathbf{v}^u$, the unit vector.  Finally, recall that our
notation for $a_xa_y$, can also be thought of as indicating a
vector $[a_x, \;\; a_y]$, and we will use the notations interchangeably.

Algorithm~\ref{alg:angle_search} formalizes the procedure outlined in steps
1-3 above, and proceeds as follows.  Line~\ref{ln:as3} adds every region vector
 $a_xa_y$ to each element of $\mathcal{S}_u$, before finding the intersection of
 the resulting vectors with the set
 of valid labels in line~\ref{ln:as4}.  With these valid label's in hand,
 line~\ref{ln:as7} reverses the operation of line~\ref{ln:as4}, and
 the resulting $\mathbf{u}$ are in fact the vectors
 displayed and color coded for each region in figure~\ref{subfig:all_vectors}.
 Finally, the angle between
 the vector $\mathbf{u}$ and the unit vector $\mathbf{v}^u$ are calculated in
 line~\ref{ln:as7}.  The trigonometric function $atan2$ is used in order to return an angle between $0$ and
 $360$ to conform to our target rotations (figure~\ref{subfig:all_vectors}).  The inputs to this function
are the dot product of $\mathbf{u}$ and $\mathbf{v}^u$ followed by the determinant of the same vectors
concatenated together--concatenation denoted by $||$, and the determinant denoted with vertical bars on
each side of the expression, $| \cdot |$.

\subsection{Indexing State By Region Label and Rotation Angles}

In order to write an analytic expression for $H$, $h$, and $V$,
 one more relationship is required: The state defined in
 section~\ref{sec:state} as a function of $a_xa_y$ and $\theta$.
  This relationship has two parts: a method of indexing 2x2
  matrices by an angle $\theta\in\{0,90,180,270\}$, and
  operations to flip the rows and columns of a 2x2 matrix.
We'll start with an intuition of the indexing scheme.
  Recall the variables of state~\ref{eq:P} and~\ref{eq:D}:
\[
  \mathbf{P} \equiv \begin{bmatrix}
    [x_{^-1}, y_{^-1}] & [x_1, y_{^-1}] \\
    [x_{^-1}, y_1] & [x_1, y_1]
  \end{bmatrix},
\]
and
\[
  \mathbf{\Delta} \equiv \begin{bmatrix}
    \delta_{^-1^-1} & \delta_{1^-1} \\
    \delta_{^-11} & \delta_{11}
    \end{bmatrix}.
\]
Note that for region $00$, $H,V$, and $h$ can all be written in terms of each term in these matrices
for each angle $\theta$ by moving clockwise starting
in the bottom right corner of each matrix
(i.e. $\mathbf{P}_{11}$ and $\mathbf{\Delta}_{11}$).  For example
at $\theta=0^o$ for region $00$, $H,V$, and $h$ will all be bounded by $[x_1,\; y_1]$ and $\delta_{11}$
(i.e. the terms of $\mathbf{P}_{11}$ and $\mathbf{\Delta}_{11}$).  This observation on the pattern of
matrix indexing according to angle rotations can be generalized for rows (denoted \textsf{r}) and columns
(denoted \textsf{c}) of matrices as
\begin{flalign}
  \textsf{c} &\equiv \mathbf{min}(1 + [R_{-\theta} \cdot \mathbf{v}^u]_y, 1)\\
  \textsf{r} &\equiv \mathbf{min}(1 + [R_{-\theta} \cdot \mathbf{v}^u]_x, 1).
\end{flalign}
To simplify notation however, for any 2x2 matrix $M$ and
angle $\theta \in \{0,90,180,360\}$, we write
\begin{equation}
   M[\theta] \equiv M_{\textsf{rc}},
   \label{eq:tidx}
\end{equation}
which allows us to directly index variables of state by angle
$\theta$.  However this function will remain limited to
region $00$ without first providing regional transformations
for the matrices of state presented in section~\ref{sec:state}.

The matrices of state given by equations
~\eqref{eq:B}--\eqref{eq:D} can be indexed by $\theta$
for all regions $a_xa_y \in \mathcal{R}$ as in
equation \eqref{eq:tidx} if they are first transformed.
This transformation is given by the following function, where
for the state matrix $M$, the
region transformation $M_{a_xa_y}$, is defined as:
\begin{equation}
  M_{a_xa_y} \equiv \begin{pmatrix}
  0 & 1\\
  1 & 0
  \end{pmatrix}^{2-|a_y|} \cdot M \cdot \begin{pmatrix}
  0 & 1\\
  1 & 0
  \end{pmatrix}^{2-|a_x|},
  \label{eq:flips}
\end{equation}
where the pre-multiplication of matrix
$\begin{pmatrix}
  0 & 1\\
  1 & 0
  \end{pmatrix}$ flips the rows of matrix $M$, and the post-multiplication
of matrix $M$ by $\begin{pmatrix}
  0 & 1\\
  1 & 0
  \end{pmatrix}$ flips its columns.  Given our relatively small state space,
this can be verified as yielding appropriate matrices of state
for each region through an exhaustion of cases.

\subsection{Uniform Region Bounds}

Taken together, equations~\ref{eq:tidx} and~\ref{eq:flips}, allow us to retrieve the variables
 of state given $\theta$ and $a_xa_y$,
\begin{eqnarray}
    \pv &= \mathbf{P}_{a_xa_y}[\theta] \label{eq:pv} \\
    \bv &= \mathbf{B}_{a_xa_y}[\theta]\\
    \delta &= \mathbf{\Delta}_{a_xa_y}[\theta]\\
    \uv &= \mathbf{U}_{a_xa_y}[\theta]. \label{eq:uv}
\end{eqnarray}
From equations~\ref{eq:pv}--~\ref{eq:uv}, we can write $h,H,V: (a_xa_y,\theta) \ra \mathbb{R}^+$,
\begin{equation}
    h(\theta,\av) = [\rnt \cdot \pv^T]_x + [\rnt \cdot \av^T]_x \cdot [\rnt \cdot \bv^T]_x -
    [\rnt \cdot \orig]_x \cdot (1 - |\rnt \cdot \av^T|_x) +
    (1 - |\rnt \cdot \av^T|_y) \cdot \mathbf{min}_{x,y}(\rnt \cdot \uv ) \cdot \delta
    \label{eq:h}
\end{equation}
\begin{equation}
    H(\theta,\av) = [\rnt \cdot \pv^T]_x + [\rnt \cdot \av^T]_x \cdot [\rnt \cdot \bv^T]_x -
    [\rnt \cdot \orig]_x \cdot (1 - |\rnt \cdot \av^T|_x) +
    |\rnt \cdot \av^T|_y \cdot [\rnt \cdot \uv ]_x \cdot \delta
    \label{eq:H}
\end{equation}
\begin{equation}
    V(\theta,\av) = [\rnt \cdot \pv^T]_y + [\rnt \cdot \av^T]_y \cdot [\rnt \cdot \bv^T]_y -
    [\rnt \cdot \orig]_y \cdot (1 - |\rnt \cdot \av^T|_y) +
    |\rnt \cdot \av^T|_x \cdot [\rnt \cdot \uv ]_y \cdot \delta
    \label{eq:V}
\end{equation}
Making these functions discrete (i.e. $h,H,V: (a_xa_y,\theta) \ra \mathbb{Z}^+$)
requires reworking equation~\ref{eq:O} with the ceiling function,
\begin{equation}
    \mathcal{O} \equiv \bigl[x_{^-1} + \ceil[\Big]{\frac{x_1 - x_{^-1}}{2}}, \;\;\;
                             y_{^-1} + \ceil[\Big]{\frac{y_1 - y_{^-1}}{2}} \bigl],
    \label{eq:discreteO}
\end{equation}
and using using it in equations~\eqref{eq:h}--\eqref{eq:V}.

Algorithm \ref{alg:updates}
puts these equations together and gives precise analytic form to
the pseudocode developed in algorithm~\ref{alg:tile_update}.
In the interest of concision, line~\ref{ln:update} omits a precise definition of a 5
point stencil, but simply states that $g(t,x,y) = f(t,x,y)$ meaning at time
$t$, we can compute the value of a stencil at $g(t,x,y)$ given some function $f$
of the same point $x,y$ at the prior time $t-1$.
Line~\ref{ln:as} executes the procedure of algorithm~\ref{alg:angle_search},
and illustrates the parallel between our notion of a tile and
it's apparent encoding as an angle.  The potential for
an intersection in the mathematical research on {\it tiling} is
intriguing, but has yet to bear analytic fruit beyond the broad
observations made in this section.
\begin{algorithm}[h]
  \setcounter{AlgoLine}{0}
  $\Theta = AngleSearch(a_xa_y)$\; \label{ln:as}
  \For{$\theta \in \Theta $}{
    \For{$j=0$ \KwTo $j=H(\theta, \av)$}{
      \For{$k=0$ \KwTo $k=\mathbf{min}\{V(\theta,\av), V(\theta,\av) - j + h(\theta, \av)$}{
        $\mathbf{i} = \rt \cdot [j, \;\; k]^T$\;
        $g(t,T_x(\av) + \mathbf{i}_x
        T_y(\av) + \mathbf{i}_y)) =
        f(t-1,T_x(\av) + \mathbf{i}_x,
        T_y(\av) + \mathbf{i}_y)$
        \label{ln:update}
      }
    }
  }

  \caption{Uniform Region Update $\forall \text{ input } a_xa_y = \av \in \mathcal{R}$}
  \label{alg:updates}
\end{algorithm}

% % NOTE This seems to work, and is more compact
% %      but is harder to understand so probably not worthwhile
% % \begin{algorithm}[h]
% %   \setcounter{AlgoLine}{0}
% %   $\Theta = \text{AngleSearch}(a_xa_y)$\;
% %   $x_{^{^-}1} = x_{^{^-}1} + (1-|a_x|\cdot|ay|)\cdot ( 1-2\cdot\textsf{max}(|a_x|,|a_y|))$\;
% %   $y_{^{^-}1} = y_{^{^-}1} + (1-|a_x|\cdot|ay|)\cdot ( 1-2\cdot\textsf{max}(|a_x|,|a_y|))$\;
% %   $x_{1} = x_{1} + (1-|a_x|\cdot|ay|)\cdot ( 2\cdot\textsf{max}(|a_x|,|a_y|) -1) $\;
% %   $y_{1} = y_{1} + (1-|a_x|\cdot|ay|)\cdot ( 2\cdot\textsf{max}(|a_x|,|a_y|) -1) $\;
% %   $\Delta' = \Delta_{a_xa_y}$\;
% %   \For{$\theta \in \Theta$}{
% %     $\Delta'[\theta] = \Delta[\theta] +
% %     2\cdot\textsf{max}(|a_x|\cdot|a_y|,(1-|a_x|)\cdot(1-|a_x|)) - 1$
% %   }
% %   \caption{Procedure to update state values for each region $a_xa_y$}
% %   \label{alg:state_updates_compact}
% % \end{algorithm}
\section{Update Constraints for the Multi-Process Problem}

Up until now we've been considering updates on a
single process using the regions decomposed by the
state
laid out in section~\ref{sec:state}, but we still
need to consider coordination between processes.
Recall in our discussion of algorithm~\ref{alg:state_updates_basic} that if the procedure is called
once for every region (i.e. $\forall a_xa_y \in \mathcal{R}$), then the borders introduced in the static
region decomposition in section~\ref{sec:problem_statement} remain unchanged.  However when this requirement
is relaxed, then a few things immediately follow.  In
particular, certain conditions must be met for
algorithm~\ref{alg:updates} to be invoked by a
process region at all.  This
section enumerates those conditions.

\subsection{Nonzero Region Area}

This first such condition is that for every region $a_xa_y \in \mathcal{R}$, and for all
$\theta\in AngleSearch(a_xa_y)$, it must be the case that $h(\theta,a_xa_y) > 0$,
$H(\theta,a_xa_y) > 0$, and $V(\theta,a_xa_y) > 0$.  This can be summarized by the
boolean function,
\begin{equation}
    \mathcal{G}^S(a_xa_y,\Theta) \equiv \underset{\theta \in \Theta}{\wedge}
    H(\theta,\mathbf{a}) > 2 \wedge
    h(\theta,\mathbf{a}) > 2 \wedge
    V(\theta,\mathbf{a}) > 2,
    \label{eq:space_guard}
\end{equation}
where $\Theta = AngleSearch(a_xa_y)$.
Here, $0$ is replaced by $2$ given that in discrete space, a 5 point stencil must have at
minimum a $3x3$ grid of cells to update a single point
(or at least 5 cells, with its 4 corners missing).  We'll refer to formula~\ref{eq:space_guard}
as the {\it spatial update guard}.

\subsection{Interprocess Communication Constraints}

Any process wishing to update its border cells must receive ghost
cells from one or more neighbor processes.
Therefore, there must be a means of ensuring that the requisite
{\it communication} (sending and receiving of data) is performed
and completed.
Given our assumption that communication completion is nondeterministic, the abstraction
we will use will mimic MPI's non-blocking communication.  Let
every process have a unique id $\pid \in \mathbb{Z}^+$.  Let
the $\mathcal{N}(a_xa_y,\pid): (a_xa_y,\pid) \ra \{\mathbb{Z}^+\}$ be a mapping
from a region $a_xa_y$ and a process id $\pid$ to a set of positive
integers which themselves are process ids of the processes that
$\pid$ must communicate with (these ids are referred to as {\it neighbors}).

Next, we define functions analogous to MPI's
$\textsf{\textup{ISend}}$ and $\textsf{\textup{IRecv}}$ respectively.
$\textsf{send}_{a_xa_y}[n] \la \psendf(n\in\mathbb{Z}^+,\textsf{d})$ is the signature for
a function which takes a neighbor process id as its first argument, and some
arbitrary data structure $\textsf{d}$ as its second argument, and returns a boolean value
in the array $\textsf{send}_{a_xa_y}[0..|\mathcal{N}(a_xa_y,\pid)|]$,
 which is initially set to false, and then
nondeterministically set to true when the neighbor process $n$ receives
the value $\textsf{d}$, and copies it to a local data structure.  Likewise,
$\textsf{rcv}_{a_xa_y}[n] \la \precvf(n\in\mathbb{Z}^+,\textsf{d})$ is the signature for
a function which takes a neighbor process id as its first argument, and some
arbitrary local data structure $\textsf{d}$ as its second argument,
and returns a boolean value
in the array $\textsf{rcv}_{a_xa_y}[0..|\mathcal{N}(a_xa_y,\pid)|]$,
 which is initially set to false, and then
nondeterministically set to true when the neighbor process
$\pid$ receives a new value in its local data structure $\textsf{d}$
from process $n$, and can perform a validly access the memory for $\textsf{d}$.

Given these definitions for communication between neighbor processes, it follows
for all $a_xa_y\in\mathcal{R} - \{00\}$ that the following boolean function
must be true for any valid update to be performed:
\begin{equation}
    \mathcal{G}^C(a_xa_y,\pid) \equiv \underset{n \in \mathcal{N}(a_xa_y,\pid)}{\wedge}
    \textsf{send}_{a_xa_y}[n] \wedge \textsf{rcv}_{a_xa_y}[n].
    \label{eq:comm_guard}
\end{equation}
We'll call this the {\it communication update guard}.

\subsection{Mirror Process Misalignment}

\begin{figure}[h]
    \centering
    \includegraphics[width=.3\textwidth]{mirrorPaperFinal.eps}
    \caption{A motivating example for the utility of $\mathcal{G_M}$.  A subset of the
    2D data arrays of a left and right process are illustrated--ghost cells drawn without a border, and
    border cells given a border.  A delimiter between regions on each process perpendicular To
     $B_x^{1}$ and $B_x^{^-1}$ is drawn for the left and right process, respectively.  Region labels
     are shown for the 4 regions of interest to the example, and the rest are omitted}
    \label{fig:mirror_motivation}
\end{figure}

Given a global spatial decomposition as described in section~\ref{sec:problem_statement},
2 neighbor processes are referred to as
{\it mirror processes} of each other.  In figure~\ref{fig:basic}, process 2 and 4 are process 1's mirror
processes, process 1 and 3 are process 2's mirror processes, and so on.  Further,
the state variables outlined in section~\ref{sec:state} are referred to as the
{\it mirror state} of the processes adjacent to them.

Figure~\ref{fig:mirror_motivation} illustrates an example how the {\it mirror state}
of 2 processes can become misaligned.  This discussion will not show how such a state
is reachable, but what must be done when it occurs in order to respect the dependency
constraints of a cell update at the border.
Drawn is a left process with regions $-10$ and $10$ shown,
and its {\it mirror process} on the right with regions $-10$ and $-1-1$.  Border cells
of the processes are bordered by a square boxes, and a timesteps are displayed within them.
Ghost cell timesteps are displayed with no border.
\begin{algorithm}[h]
  % NOTE: Mirror state has the same matrix flips as the the current state, so at least for
  % the actual implementation of things, this would need to be accounted for
  \setcounter{AlgoLine}{0}
  \Procedure{$\mathcal{G}^M(a_xa_y \in \mathcal{R}-\{00\}, \Theta)$}{
    % \When{$a_xa_y = 00$}{
    %     $\mathbf{return}\;\;\;  \textsf{true}$\;
    % }
    $\theta_1 = \Theta[0]$; $\;\;\;\theta_2 = \Theta[1] \;\mathbf{ ? }\; |\Theta| = 2 \;\mathbf{ : }\; \theta_1$\;
    $\mathbf{return }\;\; H[\theta_1] \leq V^M[\theta_1 - 90] \wedge
    V[\theta_2] \leq V^M[\theta_2 + 90] $\;
  }
  \caption{Procedure to check mirror state for each
  $a_xa_y \in \mathcal{R}$}
  \label{alg:mirror_guard}
\end{algorithm}
Observe that the border cells
of the left process are exactly reflected in the ghost cells of the right process and
vice~versa.  This in fact corresponds to the state in each process, $\mathbf{P}$ and
$\mathbf{\Delta}$.  We assume further that this state is exchanged between processes,
and stored as locally in {\it mirror variables}, where
\begin{eqnarray}
  \mathbf{\Delta}_n^M &\equiv \mathbf{\Delta}\;\text{matrix of neighbor process $n$}\\
  \mathbf{P}_n^M &\equiv \mathbf{P}\;\text{matrix of neighbor process $n$}.
\end{eqnarray}
It follows, that any calculation made with these terms will receive a superscript $M$
to denote that it is in fact measuring the term for the mirror process (e.g.
$V^M(\theta, \mathbf{a}), h^M(\theta, \mathbf{a})$).

To ensure that
a region $a_xa_y$ is not updated when its appropriate ghost cells are not present
as in region $-10$ in figure~\ref{fig:mirror_motivation}, the boolean function
$\mathcal{G}^M$ shown in algorithm~\ref{alg:mirror_guard} can be used.  The function
returns $\textsf{true}$ if the region is an interior region, and then checks alignment
with mirror quantities from its neighbor process(es).  $H(\theta,\mathbf{a})$ is
mirrored by $V^M(\theta - 90^o,\mathbf{a})$, and $V(\theta,\mathbf{a})$ is mirrored
by $V^M(\theta + 90^o,\mathbf{a})$.  This is a relationship that holds up for all
$a_xa_y \in \mathcal{R} - \{00\}$, and can be checked by drawing out the rotated bounds
$V$ and $H$ for all border and corner regions, along side their region bounds.
% TODO: Double check, but this algorithm may not be necessary at all if we're overwriting
%       mirror state for each neighbor $n$.
% \begin{algorithm}[h]
%   % % %% TODO: If corner, need to assert that the latest timestep is strictly
%   % % %%       less than or equal to the middle regions
%   \setcounter{AlgoLine}{0}
%   \Procedure{$\mathcal{G}^T(a_xa_y \in \mathcal{R}, \Theta)$}{
%     \When{$a_xa_y = 00$}{
%         $\mathbf{return}\;\;\;  \textsf{true}$\;
%     }
%     $ready = \textsf{true}$\;
%     \For{$\theta \in \Theta $}{
%       $\mathbf{i} = \rt \cdot [H(\theta,\av), V(\theta,\av)]$\;
%       % TODO: Do we need to rotate these as well to get the signs right
%       $b = \rnt \cdot B_{a_xa_y}[\theta]$\;
%       $\dx = \textsf{sign}(\mathbf{i}_x)$;
%       $\;\;\dy = \textsf{sign}(\mathbf{i}_y)$\;
%       $ready \;\wedge= a_{x+\dx}a_y \in \mathcal{R} - \{00\} \wedge
%       t(a_xa_y, T_x + b_x + \mathbf{i}_x,
%       T_y + b_y) = t_{a_{x+\dx}a_y}^M(T_x + b_x + \mathbf{i}_x + \dx,
%       T_y + b_y)$\;

%       $ready \;\wedge= a_xa_{y+\dy} \in \mathcal{R} - \{00\} \wedge
%       t(a_xa_y, T_x + b_x,
%       T_y + b_y + \mathbf{i}_y) = t_{a_xa_{y+\dy}}^M(T_x + b_x,
%       T_y + b_y + \mathbf{i}_y + \dy)$\;
%     }
%     $\mathbf{return}\;\;\;  ready$\;
%   }
%   \caption{Guard routine to check that neighbors are aligned for
%   regions $a_xa_y \in \mathcal{R}$, named $\mathcal{G}^T$}
%   \label{alg:time_guard}
% \end{algorithm}

% Image~\ref{fig:mirror_motivation} also provides a clue for how to check
% whether or not the ghost cells are sufficiently updated for a border
% region to have all of its cell dependencies met.  Note that the ghost
% cells of $-10$ and $-1-1$ violate the function~\eqref{eq:ts} on the
% boundary of the 2 regions (there is a 'plateau' in the timestep rather
% than a strictly increasing step as described by equation \eqref{eq:ts}).

% Even if mirror state happens to be stale, algorithm \ref{alg:time_guard}
% can check whether or not this relationship is

\section{Full Algorithm}

\begin{algorithm}[h]
  \setcounter{AlgoLine}{0}
  \tcc{Initialize Global Variables.}
  $\tm\la 1$\;
  $x_{^-1}\la 1$; $x_1\la B_x^1$; $y_{^-1} \la 1$; $y_1 \la B_y^1$\;
  $\delta_{^-11}\la 0$; $\delta_{^-1^-1}\la 0$; $\delta_{11}\la 0$; $\delta_{1^-1}\la 0$\;


  \For{$a_xa_y \in \mathcal{R}-\{00\}$}{ % Initialize communication
      $\Theta = \text{AngleSearch}(a_xa_y)$\;
      \For{$\theta \in \Theta $}{
          \For{$n \in \mathcal{N}(a_xa_y,\pid)$}{
              $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\mathbf{\Delta}}$;
              $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\mathbf{\Delta}^M_n}$\;
              $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\mathbf{P}}$;
              $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\mathbf{P}^M_n}$\;
              $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\data[1]
              [\mathbb{I}_y(-1, \theta,\mathbf{a})^s..\mathbb{I}_y(+1, \theta,\mathbf{a})^s]
              [\mathbb{I}_x(-1, \theta,\mathbf{a})^s..\mathbb{I}_x(+1, \theta,\mathbf{a})^s]}$;
              $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\data[0]
              [\mathbb{I}_y(-1, \theta,\mathbf{a})..\mathbb{I}_y(+1, \theta,\mathbf{a})]
              [\mathbb{I}_x(-1, \theta,\mathbf{a})..\mathbb{I}_x(+1, \theta,\mathbf{a})]}$\;
          }
      }
  }
  % While loop for state (time and space)
  \While(){$\tm < \textsf{nsteps} \vee x_{^-1} > 0 \vee y_{^-1} > 0 \vee x_{1} < B_x^1 \vee y_{^-1} > B_y^1$}{
      \For{$a_xa_y \in \mathcal{R}$}{
          $\Theta = \text{AngleSearch}(a_xa_y)$\; \label{ln:fas}

          \When{$ \mathcal{G}^S(a_xa_y, \Theta) \wedge (
              (
                  \mathcal{G}^C(a_xa_y,\pid) \wedge
                  \mathcal{G}^M(a_xa_y, \Theta)
              ) \vee a_xa_y = 00
              )$}{
              $\theta_c = 0$\;
              \For{$\theta \in \Theta $}{
                  $\pv = \mathbf{P}_{a_xa_y}[\theta]$\;
                  $\bv = \mathbf{B}_{a_xa_y}[\theta]$\;
                  $\delta = \mathbf{\Delta}_{a_xa_y}[\theta]$\;
                  $\uv = \mathbf{U}_{a_xa_y}[\theta]$\;
                  $h(\theta,\av) = [\rnt \cdot \pv^T]_x + [\rnt \cdot \av^T]_x \cdot [\rnt \cdot \bv^T]_x -
                  [\rnt \cdot \orig]_x \cdot (1 - |\rnt \cdot \av^T|_x) +
                  (1 - |\rnt \cdot \av^T|_y) \cdot \mathbf{min}_{x,y}(\rnt \cdot \uv ) \cdot \delta$\;
                  $H(\theta,\av) = [\rnt \cdot \pv^T]_x + [\rnt \cdot \av^T]_x \cdot [\rnt \cdot \bv^T]_x -
                  [\rnt \cdot \orig]_x \cdot (1 - |\rnt \cdot \av^T|_x) +
                  |\rnt \cdot \av^T|_y \cdot [\rnt \cdot \uv ]_x \cdot \delta$\;
                  $V(\theta,\av) = [\rnt \cdot \pv^T]_y + [\rnt \cdot \av^T]_y \cdot [\rnt \cdot \bv^T]_y -
                  [\rnt \cdot \orig]_y \cdot (1 - |\rnt \cdot \av^T|_y) +
                  |\rnt \cdot \av^T|_x \cdot [\rnt \cdot \uv ]_y \cdot \delta$\;
                  $\mathbf{q} = \rt \cdot [1, \;\; 1]^T$\;
                  \For{$j=0$ \KwTo $j=H(\theta, \av) - \mathbf{q}_x$}{
                      \For{$k=0$ \KwTo $k=\mathbf{min}\{V(\theta,\av) - \mathbf{q}_y, V(\theta,\av) - j + h(\theta, \av) - \mathbf{q}_y\}$}{
                          $\mathbf{i} = \rt \cdot [j, \;\; k]^T$\;
                          $\data[1-\overline{t(a_xa_y,j,k)}]
                          [T_y(\av) + \mathbf{i}_y) + \mathbf{q}_y \cdot (1-|a_y|) \cdot \mathbf{min}((\theta_c \;\mathbf{mod}\; 3),1)]
                          [T_x(\av) + \mathbf{i}_x + \mathbf{q}_x \cdot (1-|a_x|) \cdot \overline{\theta_c}]
                          =
                          f'(\data[\overline{t(a_xa_y,j,k)}]
                          [T_y(\av) + \mathbf{i}_y + \mathbf{q}_y \cdot (1-|a_y|) \cdot \mathbf{min}((\theta_c \;\mathbf{mod}\; 3),1)]
                          [T_x(\av) + \mathbf{i}_x + \mathbf{q}_x \cdot (1-|a_x|) \cdot \overline{\theta_c}])$
                          \label{ln:fupdate}
                      }
                  }
                  $\theta_c = \theta_c + 1$\;
              }
              \When{$a_xa_y \neq 00$}{
                  \For{$\theta \in \Theta $}{
                      \For{$n \in \mathcal{N}(a_xa_y,\pid)$}{
                          $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\mathbf{\Delta}}$;
                          $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\mathbf{\Delta}^M_n}$\;
                          $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\mathbf{P}}$;
                          $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\mathbf{P}^M_n}$\;
                          $\textsf{send}_{a_xa_y}[n] \la \psend{n}{\data[1-\overline{t(a_xa_y,j,k)}]
                          [\mathbb{I}_y(-1, \theta,\mathbf{a})^s..\mathbb{I}_y(+1, \theta,\mathbf{a})^s]
                          [\mathbb{I}_x(-1, \theta,\mathbf{a})^s..\mathbb{I}_x(+1, \theta,\mathbf{a})^s]}$;
                          $\;\;\textsf{rcv}_{a_xa_y}[n] \la \precv{n}{\data[\overline{t(a_xa_y,j,k)}]
                          [\mathbb{I}_y(-1, \theta,\mathbf{a})..\mathbb{I}_y(+1, \theta,\mathbf{a})]
                          [\mathbb{I}_x(-1, \theta,\mathbf{a})..\mathbb{I}_x(+1, \theta,\mathbf{a})]}$\;
                      }
                  }
              }
              \When{$a_xa_y = 00$}{
                  $\tm \la \tm + 1$\;
              }
              $update\_state(a_xa_y)$\;
          }
      }
  }
  \caption{Full algorithm to be run on every process until termination conditions are met}
  \label{alg:full}
\end{algorithm}

The full description for a parallel decomposition of a 5 point stencil
is provided in algorithm \ref{alg:full}.  Some details that haven't been
explored are provided here for completeness.
To start with, how to handle discrete space indexing given rotations
within a region $a_xa_y$.  The modulo terms below account for this (where
$r = x \;\mathbf{mod}\; y$, and $r$ is the remainder from dividing
$x$ into $y$).  Given its frequency, the shorthand
 $\overline{x} = x \;\mathbf{mod}\; 2$ is used as well when $y=2$.
Additionally, a termination condition has not been introduced for the
algorithm.  Recall that the modeling problem initially set out in section
\ref{sec:problem_statement} is that of PDEs, which evolves a spatially dependent
quantity over time.  Therefore the stopping condition will be the number of
timesteps to evolve the spatial property over, $\textsf{nsteps}$.

Finally, we will actually introduce operations on an array of cells being used
to model the PDE.  $\data[0..1][0..B_y^1+1][0..B_x^1+1]$ will
hold all the grid cells regardless of region, including the ghost cells.  It
is a 3D array with a 'double buffering' strategy for updating in time
in the first dimension, and the height and length of the local portion
of the process' PDE stored in the rows and columns, respectively.  For
 a set of shorthands for defining the bounds to send and receive in
 each spatial dimension of $\data$ we define
\begin{eqnarray}
    \mathbb{I}_x(q, \theta,\mathbf{a}) &= T_x + |a_x|\cdot (a_x - q) \cdot \rt \cdot [H(\theta,\mathbf{a}),V(\theta,\mathbf{a})]^T_x\\
    \mathbb{I}_y(q, \theta,\mathbf{a}) &= T_y + |a_y|\cdot (a_y - q) \cdot \rt \cdot [H(\theta,\mathbf{a}),V(\theta,\mathbf{a})]^T_y,
\end{eqnarray}
where $q\in \{-1, 1\}$ indicating the left bound and right bound respectively.
This is a discrete function which works for the receive buffers (ghost cells), but given
we're also accessing send buffers (border cells), this won't be sufficient.
This combined with the need to account for the case where
$\mathbb{I}_x(-1,\theta,\mathbf{a}) = \mathbb{I}_x(+1,\theta,\mathbf{a})$ motivates
the additional helper function
\begin{eqnarray}
    \mathbb{I}_x(q,\theta,\mathbf{a})^s &= \textbf{min}(\textbf{max}(\mathbb{I}_x(q,\theta,\mathbf{a})-q,1),B_x^1)\\
    \mathbb{I}_y(q,\theta,\mathbf{a})^s &= \textbf{min}(\textbf{max}(\mathbb{I}_y(q,\theta,\mathbf{a})-q,1),B_y^1).
\end{eqnarray}
Finally, note that we do not define the entire stencil update
method for a 5 point stencil, but continue to denote it by
$f'$, which no longer needs a variable of time
(as in algorithm~\ref{alg:updates}), as our
$\data$ buffer has made this access precise.  Further,
$g$ has been dropped from the original formulation of
algorithm
\ref{alg:updates} as the left hand side of the update
need only access
the single appropriate center cell of the 5-point
dependency structure under study.